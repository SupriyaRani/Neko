{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2370593526.py, line 7)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[1], line 7\u001b[1;36m\u001b[0m\n\u001b[1;33m    sudo apt update\u001b[0m\n\u001b[1;37m         ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# !pip install kivy pillow\n",
    "# !pip install opencv-python numpy matplotlib\n",
    "# !pip install ultralytics\n",
    "# !pip install --upgrade pip setuptools\n",
    "# !pip install buildozer\n",
    "# !pip install --upgrade buildozer cython\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip uninstall opencv-python\n",
    "# !pip install opencv-python==4.5.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('yolov3.cfg', <http.client.HTTPMessage at 0x13799e78c10>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib.request\n",
    "\n",
    "# Download YOLOv3 weights\n",
    "urllib.request.urlretrieve(\n",
    "    \"https://pjreddie.com/media/files/yolov3.weights\", \n",
    "    \"yolov3.weights\"\n",
    ")\n",
    "\n",
    "# Download YOLOv3 config\n",
    "urllib.request.urlretrieve(\n",
    "    \"https://raw.githubusercontent.com/pjreddie/darknet/master/cfg/yolov3.cfg\", \n",
    "    \"yolov3.cfg\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e:\\Neko\\yolo.cfg\n",
      "e:\\Neko\\yolo.weights\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.path.abspath(\"yolo.cfg\"))\n",
    "print(os.path.abspath(\"yolo.weights\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kivy.app import App\n",
    "from kivy.uix.boxlayout import BoxLayout\n",
    "from kivy.uix.button import Button\n",
    "from kivy.uix.image import Image\n",
    "from kivy.uix.filechooser import FileChooserListView\n",
    "from kivy.graphics.texture import Texture\n",
    "from PIL import Image as PILImage\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load YOLO Model\n",
    "yolo_model = \"yolov3.weights\"  # Replace with \"yolov8.onnx\" if using YOLOv8\n",
    "yolo_cfg = \"yolov3.cfg\"\n",
    "coco_labels = \"coco.names\"  # File containing COCO dataset class labels\n",
    "\n",
    "# Load class labels\n",
    "with open(coco_labels, 'r') as f:\n",
    "    classes = f.read().strip().split(\"\\n\")\n",
    "\n",
    "# Initialize the YOLO model\n",
    "net = cv2.dnn.readNetFromDarknet(yolo_cfg, yolo_model)\n",
    "layer_names = net.getLayerNames()\n",
    "# Safely get output layers based on OpenCV version\n",
    "output_indices = net.getUnconnectedOutLayers()\n",
    "output_layers = [layer_names[i - 1] for i in output_indices.flatten()]\n",
    "\n",
    "\n",
    "# Load the image\n",
    "image = cv2.imread('NekoImages\\pic.jpg')\n",
    "(H, W) = image.shape[:2]\n",
    "\n",
    "# Prepare image for YOLO\n",
    "blob = cv2.dnn.blobFromImage(image, scalefactor=1 / 255.0, size=(416, 416), swapRB=True, crop=False)\n",
    "net.setInput(blob)\n",
    "\n",
    "# Perform object detection\n",
    "layer_outputs = net.forward(output_layers)\n",
    "\n",
    "# Process detections\n",
    "boxes = []\n",
    "confidences = []\n",
    "class_ids = []\n",
    "\n",
    "confidence_threshold = 0.5\n",
    "nms_threshold = 0.3\n",
    "\n",
    "for output in layer_outputs:\n",
    "    for detection in output:\n",
    "        scores = detection[5:]  # Class scores start from index 5\n",
    "        class_id = np.argmax(scores)\n",
    "        confidence = scores[class_id]\n",
    "\n",
    "        if confidence > confidence_threshold:\n",
    "            box = detection[0:4] * np.array([W, H, W, H])\n",
    "            (center_x, center_y, width, height) = box.astype(\"int\")\n",
    "\n",
    "            # Calculate top-left corner of the bounding box\n",
    "            x = int(center_x - (width / 2))\n",
    "            y = int(center_y - (height / 2))\n",
    "\n",
    "            boxes.append([x, y, int(width), int(height)])\n",
    "            confidences.append(float(confidence))\n",
    "            class_ids.append(class_id)\n",
    "\n",
    "# Apply Non-Maxima Suppression (NMS)\n",
    "indices = cv2.dnn.NMSBoxes(boxes, confidences, confidence_threshold, nms_threshold)\n",
    "\n",
    "# Draw detected boxes on the image\n",
    "if len(indices) > 0:\n",
    "    for i in indices.flatten():\n",
    "        x, y, w, h = boxes[i]\n",
    "        label = f\"{classes[class_ids[i]]}: {confidences[i]:.2f}\"\n",
    "        \n",
    "        # Draw bounding box and label\n",
    "        cv2.rectangle(image, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "        # cv2.putText(image, label, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)\n",
    "\n",
    "# Display image with bounding boxes\n",
    "cv2.imshow('Detected Objects', image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Assuming `boxes` contains the bounding box coordinates as a result of detection\n",
    "# boxes format: [(x1, y1, w1, h1), (x2, y2, w2, h2), ...]\n",
    "\n",
    "# Create a blank mask with the same dimensions as the input image\n",
    "mask = np.zeros((image.shape[0], image.shape[1]), dtype=np.uint8)\n",
    "\n",
    "# Loop through the bounding boxes and fill them on the mask\n",
    "for (x, y, w, h) in boxes:\n",
    "    # Draw filled rectangles corresponding to the bounding boxes\n",
    "    cv2.rectangle(mask, (x, y), (x + w, y + h), 255, -1)  # White filled rectangle\n",
    "\n",
    "# Display the generated mask\n",
    "cv2.imshow('Object Mask', mask)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Assume you already have:\n",
    "# `image`: Loaded input image\n",
    "# `mask`: Binary mask of the detected objects (255 for objects, 0 elsewhere)\n",
    "\n",
    "# Step 1: Copy the image for the text overlay\n",
    "text_layer = np.zeros_like(image, dtype=np.uint8)  # Create a blank image for the text\n",
    "image_with_text = image.copy()\n",
    "\n",
    "# Step 2: Add text to the blank text layer\n",
    "text = \"Text Behind Objects\"\n",
    "text_position = (130, 460)\n",
    "cv2.putText(text_layer, text, text_position, cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
    "\n",
    "# Step 3: Invert the mask to make objects opaque\n",
    "inverted_mask = cv2.bitwise_not(mask)\n",
    "\n",
    "# Step 4: Use the mask to remove parts of the text behind the objects\n",
    "text_visible = cv2.bitwise_and(text_layer, text_layer, mask=inverted_mask)\n",
    "\n",
    "# Step 5: Combine the original image with the visible text\n",
    "result = cv2.addWeighted(image_with_text, 1, text_visible, 1, 0)\n",
    "\n",
    "# Step 6: Display the final image\n",
    "cv2.imshow('Image with Text Behind Objects', result)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 e:\\Neko\\NekoImages\\pic.jpg: 640x480 3 persons, 1 cell phone, 3188.5ms\n",
      "Speed: 46.2ms preprocess, 3188.5ms inference, 168.7ms postprocess per image at shape (1, 3, 640, 480)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !pip install ultralytics\n",
    "from ultralytics import YOLO\n",
    "from PIL import Image\n",
    "model = YOLO(\"yolov8m-seg.pt\")\n",
    "results = model.predict(\"NekoImages\\pic.jpg\")\n",
    "result = results[0]\n",
    "masks = result.masks\n",
    "len(masks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 e:\\Neko\\NekoImages\\pic.jpg: 640x480 3 persons, 1 cell phone, 950.3ms\n",
      "Speed: 0.0ms preprocess, 950.3ms inference, 9.9ms postprocess per image at shape (1, 3, 640, 480)\n"
     ]
    }
   ],
   "source": [
    "# !pip install ultralytics\n",
    "from ultralytics import YOLO\n",
    "from PIL import Image\n",
    "model = YOLO(\"yolov8m-seg.pt\")\n",
    "results = model.predict(\"NekoImages\\pic.jpg\")\n",
    "result = results[0]\n",
    "masks = result.masks\n",
    "len(masks)\n",
    "mask1 = masks[0]\n",
    "mask = mask1.data[0].numpy()\n",
    "polygon = mask1.xy[0]\n",
    "mask_img = Image.fromarray(mask,\"I\")\n",
    "# mask_img.show()\n",
    "from PIL import ImageDraw\n",
    "img = Image.open(\"NekoImages\\pic.jpg\")\n",
    "draw = ImageDraw.Draw(img)\n",
    "draw.polygon(polygon,outline=(0,255,0), width=5)\n",
    "img.show()\n",
    "mask2 = masks[1]\n",
    "mask = mask2.data[0].numpy()\n",
    "polygon = mask2.xy[0]\n",
    "mask_img = Image.fromarray(mask,\"I\")\n",
    "# mask_img.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "drawing segment on one object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 e:\\Neko\\NekoImages\\pic.jpg: 640x480 3 persons, 1 cell phone, 1105.4ms\n",
      "Speed: 7.1ms preprocess, 1105.4ms inference, 10.1ms postprocess per image at shape (1, 3, 640, 480)\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "# Load the YOLOv8 segmentation model\n",
    "model = YOLO(\"yolov8m-seg.pt\")\n",
    "\n",
    "# Predict the results from the image\n",
    "results = model.predict(\"NekoImages\\pic.jpg\")\n",
    "\n",
    "# Extract results from the first prediction\n",
    "result = results[0]\n",
    "masks = result.masks\n",
    "\n",
    "# Load the input image\n",
    "img = Image.open(\"NekoImages\\pic.jpg\")\n",
    "draw = ImageDraw.Draw(img)\n",
    "\n",
    "# Loop through each detection's mask and draw polygons for each\n",
    "for mask_idx in range(len(masks)):\n",
    "    # Get the mask data and polygon coordinates\n",
    "    mask = masks[mask_idx].data[0].numpy()\n",
    "    polygon = masks[mask_idx].xy[0]\n",
    "\n",
    "    # Draw the polygon on the image (we use green for the outline, and width = 5)\n",
    "    draw.polygon(polygon, outline=(0, 255, 0), width=5)\n",
    "\n",
    "# Display the image with the polygons\n",
    "img.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "drawing polygon on all objects detected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !pip install onnxruntime\n",
    "import onnxruntime as ort\n",
    "\n",
    "model = ort.InferenceSession(\"yolov8m-seg.onnx\")\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "img = Image.open(\"NekoImages\\pic.jpg\")\n",
    "\n",
    "# save original image size for future\n",
    "img_width, img_height = img.size;\n",
    "# convert image to RGB,\n",
    "img = img.convert(\"RGB\");\n",
    "# resize to 640x640\n",
    "img = img.resize((640,640))\n",
    "img.show()\n",
    "# convert the image to tensor \n",
    "# of [1,3,640,640] as required for \n",
    "# the model input\n",
    "input = np.array(img)\n",
    "input = input.transpose(2,0,1)\n",
    "input = input.reshape(1,3,640,640).astype('float32')\n",
    "input = input/255.0\n",
    "outputs = model.get_outputs()\n",
    "len(outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "Output0: (1, 116, 8400) Output1: (1, 32, 160, 160)\n",
      "Output0: (8400, 116) Output1: (32, 160, 160)\n",
      "Boxes: (8400, 84) Masks: (8400, 32)\n",
      "(8400, 32) (32, 25600)\n",
      "(8400, 25600)\n",
      "(8400, 25684)\n"
     ]
    }
   ],
   "source": [
    "# !pip install ultralytics\n",
    "# !pip install onnxruntime\n",
    "# !pip install opencv-python\n",
    "# !pip install numpy\n",
    "# !pip install pillow\n",
    "# Export YOLOv8 model to ONNX\n",
    "# model = YOLO(\"yolov8m-seg.pt\")\n",
    "# model.export(format=\"onnx\")\n",
    "\n",
    "from ultralytics import YOLO\n",
    "import onnxruntime as ort\n",
    "from PIL import Image, ImageDraw\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "img = Image.open(\"NekoImages\\pic.jpg\")\n",
    "img_width, img_height = img.size\n",
    "img = img.convert(\"RGB\")\n",
    "img = img.resize((640,640))\n",
    "\n",
    "input = np.array(img)\n",
    "input = input.transpose(2, 0, 1)\n",
    "input = input.reshape(1,3,640,640).astype(\"float32\")\n",
    "input = input/255.0\n",
    "input.shape\n",
    "\n",
    "# Run the model\n",
    "model = ort.InferenceSession(\"yolov8m-seg.onnx\")\n",
    "outputs = model.run(None, {\"images\":input})\n",
    "print(len(outputs))\n",
    "\n",
    "# Process the outputs\n",
    "\n",
    "output0 = outputs[0]\n",
    "output1 = outputs[1]\n",
    "print(\"Output0:\",output0.shape,\"Output1:\",output1.shape)\n",
    "\n",
    "output0 = output0[0].transpose()\n",
    "output1 = output1[0]\n",
    "print(\"Output0:\",output0.shape,\"Output1:\",output1.shape)\n",
    "boxes = output0[:,0:84]\n",
    "masks = output0[:,84:]\n",
    "print(\"Boxes:\",boxes.shape,\"Masks:\",masks.shape)\n",
    "\n",
    "output1 = output1.reshape(32,160*160)\n",
    "print(masks.shape,output1.shape)\n",
    "# Join matrices by matrix multiplication\n",
    "masks = masks @ output1\n",
    "print(masks.shape)\n",
    "# Connect masks to boxes: move all columns from masks matrix to the right of boxes matrix\n",
    "boxes = np.hstack([boxes,masks])\n",
    "print(boxes.shape)\n",
    "\n",
    "# Parse \"boxes\" to get list of bounding boxes of detected objects,\n",
    "# their segmentation masks and bounding polygons\n",
    "\n",
    "yolo_classes = [\n",
    "    \"person\", \"bicycle\", \"car\", \"motorcycle\", \"airplane\", \"bus\", \"train\", \"truck\", \"boat\",\n",
    "    \"traffic light\", \"fire hydrant\", \"stop sign\", \"parking meter\", \"bench\", \"bird\", \"cat\", \"dog\", \"horse\",\n",
    "    \"sheep\", \"cow\", \"elephant\", \"bear\", \"zebra\", \"giraffe\", \"backpack\", \"umbrella\", \"handbag\", \"tie\",\n",
    "    \"suitcase\", \"frisbee\", \"skis\", \"snowboard\", \"sports ball\", \"kite\", \"baseball bat\", \"baseball glove\",\n",
    "    \"skateboard\", \"surfboard\", \"tennis racket\", \"bottle\", \"wine glass\", \"cup\", \"fork\", \"knife\", \"spoon\",\n",
    "    \"bowl\", \"banana\", \"apple\", \"sandwich\", \"orange\", \"broccoli\", \"carrot\", \"hot dog\", \"pizza\", \"donut\",\n",
    "    \"cake\", \"chair\", \"couch\", \"potted plant\", \"bed\", \"dining table\", \"toilet\", \"tv\", \"laptop\", \"mouse\",\n",
    "    \"remote\", \"keyboard\", \"cell phone\", \"microwave\", \"oven\", \"toaster\", \"sink\", \"refrigerator\", \"book\",\n",
    "    \"clock\", \"vase\", \"scissors\", \"teddy bear\", \"hair drier\", \"toothbrush\"\n",
    "]\n",
    "\n",
    "def intersection(box1,box2):\n",
    "    box1_x1,box1_y1,box1_x2,box1_y2 = box1[:4]\n",
    "    box2_x1,box2_y1,box2_x2,box2_y2 = box2[:4]\n",
    "    x1 = max(box1_x1,box2_x1)\n",
    "    y1 = max(box1_y1,box2_y1)\n",
    "    x2 = min(box1_x2,box2_x2)\n",
    "    y2 = min(box1_y2,box2_y2)\n",
    "    return (x2-x1)*(y2-y1) \n",
    "\n",
    "def union(box1,box2):\n",
    "    box1_x1,box1_y1,box1_x2,box1_y2 = box1[:4]\n",
    "    box2_x1,box2_y1,box2_x2,box2_y2 = box2[:4]\n",
    "    box1_area = (box1_x2-box1_x1)*(box1_y2-box1_y1)\n",
    "    box2_area = (box2_x2-box2_x1)*(box2_y2-box2_y1)\n",
    "    return box1_area + box2_area - intersection(box1,box2)\n",
    "\n",
    "def iou(box1,box2):\n",
    "    return intersection(box1,box2)/union(box1,box2)\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1/(1 + np.exp(-z))\n",
    "\n",
    "# parse segmentation mask\n",
    "def get_mask(row, box, img_width, img_height):\n",
    "    # convert mask to image (matrix of pixels)\n",
    "    mask = row.reshape(160,160)\n",
    "    mask = sigmoid(mask)\n",
    "    mask = (mask > 0.5).astype(\"uint8\")*255\n",
    "    # crop the object defined by \"box\" from mask\n",
    "    x1,y1,x2,y2 = box\n",
    "    mask_x1 = round(x1/img_width*160)\n",
    "    mask_y1 = round(y1/img_height*160)\n",
    "    mask_x2 = round(x2/img_width*160)\n",
    "    mask_y2 = round(y2/img_height*160)\n",
    "    mask = mask[mask_y1:mask_y2,mask_x1:mask_x2]\n",
    "    # resize the cropped mask to the size of object\n",
    "    img_mask = Image.fromarray(mask,\"L\")\n",
    "    img_mask = img_mask.resize((round(x2-x1),round(y2-y1)))\n",
    "    mask = np.array(img_mask)\n",
    "    return mask\n",
    "\n",
    "# calculate bounding polygon from mask\n",
    "def get_polygon(mask):\n",
    "    contours = cv2.findContours(mask, cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    polygon = [[contour[0][0],contour[0][1]] for contour in contours[0][0]]\n",
    "    return polygon\n",
    "\n",
    "# parse and filter all boxes\n",
    "objects = []\n",
    "for row in boxes:\n",
    "    xc,yc,w,h = row[:4]\n",
    "    x1 = (xc-w/2)/640*img_width\n",
    "    y1 = (yc-h/2)/640*img_height\n",
    "    x2 = (xc+w/2)/640*img_width\n",
    "    y2 = (yc+h/2)/640*img_height\n",
    "    prob = row[4:84].max()\n",
    "    if prob < 0.5:\n",
    "        continue\n",
    "    class_id = row[4:84].argmax()\n",
    "    label = yolo_classes[class_id]\n",
    "    mask = get_mask(row[84:25684], (x1,y1,x2,y2), img_width, img_height)\n",
    "    polygon = get_polygon(mask)\n",
    "    objects.append([x1,y1,x2,y2,label,prob,mask,polygon])\n",
    "\n",
    "\n",
    "# apply non-maximum suppression\n",
    "objects.sort(key=lambda x: x[5], reverse=True)\n",
    "result = []\n",
    "while len(objects)>0:\n",
    "    result.append(objects[0])\n",
    "    objects = [object for object in objects if iou(object,objects[0])<0.7]\n",
    "\n",
    "len(result)\n",
    "\n",
    "# display bounding polygons of detected objects\n",
    "# on the image\n",
    "img = Image.open(\"NekoImages\\pic.jpg\")\n",
    "draw = ImageDraw.Draw(img, \"RGBA\")\n",
    "for object in result:\n",
    "    [x1, y1, x2, y2, label, prob, mask, polygon] = object\n",
    "    # move polygon from (0,0) to the top left point of detected object\n",
    "    polygon = [(round(x1+point[0]),round(y1+point[1])) for point in polygon]\n",
    "    draw.polygon(polygon, fill=(0, 255, 0, 125))\n",
    "\n",
    "img.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "drawing segment on 2 object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 e:\\Neko\\NekoImages\\pic.jpg: 640x480 3 persons, 1 cell phone, 950.2ms\n",
      "Speed: 10.1ms preprocess, 950.2ms inference, 23.6ms postprocess per image at shape (1, 3, 640, 480)\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "from PIL import Image, ImageDraw\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "# Load the YOLOv8 segmentation model\n",
    "model = YOLO(\"yolov8m-seg.pt\")\n",
    "\n",
    "# Predict the results from the image\n",
    "results = model.predict(\"NekoImages\\\\pic.jpg\")\n",
    "\n",
    "# Extract results from the first prediction\n",
    "result = results[0]\n",
    "masks = result.masks\n",
    "\n",
    "# Load the input image\n",
    "img_path = \"NekoImages\\\\pic.jpg\"\n",
    "img = Image.open(img_path)\n",
    "draw = ImageDraw.Draw(img)\n",
    "\n",
    "# Add text layer\n",
    "text_layer = np.zeros_like(np.array(img), dtype=np.uint8)  # Create a blank image for the text (1-channel, uint8)\n",
    "text_position = (130, 460)\n",
    "text = \"Text Behind Objects\"\n",
    "\n",
    "# Convert image to BGR for OpenCV, as the text will be written on BGR image\n",
    "img_cv2 = np.array(img)\n",
    "img_cv2 = cv2.cvtColor(img_cv2, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "# Step 1: Add text to the blank text layer (single channel)\n",
    "cv2.putText(text_layer, text, text_position, cv2.FONT_HERSHEY_SIMPLEX, 2, (255), 4)  # Set text color to white and increase size\n",
    "\n",
    "# Create a mask for the detected objects\n",
    "object_mask = np.zeros((img_cv2.shape[0], img_cv2.shape[1]), dtype=np.uint8)\n",
    "\n",
    "# Step 2: Loop through the mask detections and draw polygons around the detected areas\n",
    "for mask_idx in range(len(masks)):\n",
    "    # Get the mask data and polygon coordinates for each detected object\n",
    "    mask = masks[mask_idx].data[0].numpy()\n",
    "    polygon = masks[mask_idx].xy[0]\n",
    "\n",
    "    # Draw the polygon on the image (green outline, width = 5)\n",
    "    draw.polygon(polygon, outline=(0, 255, 0), width=5)\n",
    "\n",
    "    # Loop through and create a mask for the detected area\n",
    "    for point in polygon:\n",
    "        x, y = point\n",
    "        x, y = int(x), int(y)  # Convert coordinates to integers to avoid IndexError\n",
    "        if 0 <= x < img_cv2.shape[1] and 0 <= y < img_cv2.shape[0]:\n",
    "            object_mask[y, x] = 255  # Set pixels within object to 255 for visibility\n",
    "\n",
    "# Invert the object mask\n",
    "inverted_mask = cv2.bitwise_not(object_mask)\n",
    "\n",
    "# Step 3: Use the inverted mask to overlay the text layer on the original image\n",
    "# The text layer must match the shape of the image (just like the mask)\n",
    "text_layer_single_channel = cv2.cvtColor(text_layer, cv2.COLOR_BGR2GRAY)  # Convert to 1-channel grayscale if needed\n",
    "\n",
    "# Ensure the mask and text layer have the same dimensions\n",
    "inverted_mask = cv2.resize(inverted_mask, (text_layer_single_channel.shape[1], text_layer_single_channel.shape[0]))\n",
    "\n",
    "# Apply the inverted mask to the text layer\n",
    "text_visible = cv2.bitwise_and(text_layer_single_channel, text_layer_single_channel, mask=inverted_mask)\n",
    "\n",
    "# Convert the text_visible to 3 channels to match img_cv2\n",
    "text_visible = cv2.cvtColor(text_visible, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "# Final step: Combine the image with text behind objects and show result\n",
    "result_image = cv2.addWeighted(img_cv2, 1, text_visible, 1, 0)\n",
    "\n",
    "# Convert result image back to PIL for display\n",
    "result_image_pil = Image.fromarray(cv2.cvtColor(result_image, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "# Display the final image with text behind objects and polygons\n",
    "result_image_pil.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 e:\\Neko\\NekoImages\\pic.jpg: 640x480 3 persons, 1 cell phone, 928.5ms\n",
      "Speed: 7.0ms preprocess, 928.5ms inference, 10.0ms postprocess per image at shape (1, 3, 640, 480)\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "from PIL import Image, ImageDraw\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "# Load the YOLOv8 segmentation model\n",
    "model = YOLO(\"yolov8m-seg.pt\")\n",
    "\n",
    "# Predict the results from the image\n",
    "results = model.predict(\"NekoImages\\\\pic.jpg\")\n",
    "\n",
    "# Extract results from the first prediction\n",
    "result = results[0]\n",
    "masks = result.masks\n",
    "\n",
    "# Load the input image\n",
    "img_path = \"NekoImages\\\\pic.jpg\"\n",
    "img = Image.open(img_path)\n",
    "img_cv2 = np.array(img)\n",
    "img_cv2 = cv2.cvtColor(img_cv2, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "# Step 1: Create masks for the segmented objects\n",
    "object_mask = np.zeros((img_cv2.shape[0], img_cv2.shape[1]), dtype=np.uint8)\n",
    "\n",
    "for mask_idx in range(len(masks)):\n",
    "    mask = masks[mask_idx].data[0].numpy()\n",
    "    polygon = masks[mask_idx].xy[0]\n",
    "\n",
    "    for point in polygon:\n",
    "        x, y = point\n",
    "        x, y = int(x), int(y)\n",
    "        if 0 <= x < img_cv2.shape[1] and 0 <= y < img_cv2.shape[0]:\n",
    "            object_mask[y, x] = 255\n",
    "\n",
    "# Display image with bounding boxes\n",
    "cv2.imshow('Detected Objects', img_cv2)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the input image\n",
    "img = Image.open(\"NekoImages\\pic.jpg\")\n",
    "draw = ImageDraw.Draw(img)\n",
    "\n",
    "# Loop through each detection's mask and draw polygons for each\n",
    "for mask_idx in range(len(masks)):\n",
    "    # Get the mask data and polygon coordinates\n",
    "    mask = masks[mask_idx].data[0].numpy()\n",
    "    polygon = masks[mask_idx].xy[0]\n",
    "\n",
    "    # Draw the polygon on the image (we use green for the outline, and width = 5)\n",
    "    draw.polygon(polygon, outline=(0, 255, 0), width=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Assuming `boxes` contains the bounding box coordinates as a result of detection\n",
    "# boxes format: [(x1, y1, w1, h1), (x2, y2, w2, h2), ...]\n",
    "\n",
    "# Create a blank mask with the same dimensions as the input image\n",
    "# mask = np.zeros((img_cv2.shape[0], img_cv2.shape[1]), dtype=np.uint8)\n",
    "\n",
    "# # Loop through the bounding boxes and fill them on the mask\n",
    "# for (x, y, w, h) in boxes:\n",
    "#     # Draw filled rectangles corresponding to the bounding boxes\n",
    "#     cv2.rectangle(mask, (x, y), (x + w, y + h), 255, -1)  # White filled rectangle\n",
    "\n",
    "# Display the generated mask\n",
    "cv2.imshow('Object Mask', mask)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Assume you already have:\n",
    "# `image`: Loaded input image\n",
    "# `mask`: Binary mask of the detected objects (255 for objects, 0 elsewhere)\n",
    "\n",
    "# Step 1: Copy the image for the text overlay\n",
    "text_layer = np.zeros_like(img_cv2, dtype=np.uint8)  # Create a blank image for the text\n",
    "image_with_text = image.copy()\n",
    "\n",
    "# Step 2: Add text to the blank text layer\n",
    "text = \"Text Behind Objects\"\n",
    "text_position = (130, 460)\n",
    "cv2.putText(text_layer, text, text_position, cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
    "\n",
    "# Step 3: Invert the mask to make objects opaque\n",
    "inverted_mask = cv2.bitwise_not(mask)\n",
    "\n",
    "# Step 4: Use the mask to remove parts of the text behind the objects\n",
    "text_visible = cv2.bitwise_and(text_layer, text_layer, mask=inverted_mask)\n",
    "\n",
    "# Step 5: Combine the original image with the visible text\n",
    "result = cv2.addWeighted(image_with_text, 1, text_visible, 1, 0)\n",
    "\n",
    "# Step 6: Display the final image\n",
    "cv2.imshow('Image with Text Behind Objects', result)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Create a blank image for the text, drawing the text layer in white\n",
    "text_layer = np.zeros_like(img_cv2, dtype=np.uint8)\n",
    "text_position = (130, 460)\n",
    "text = \"Text Behind Objects\"\n",
    "cv2.putText(text_layer, text, text_position, cv2.FONT_HERSHEY_SIMPLEX, 2, (255, 255, 255), 4)\n",
    "\n",
    "# Step 3: Invert the object mask to make the objects opaque and background transparent\n",
    "inverted_mask = cv2.bitwise_not(object_mask)\n",
    "\n",
    "# Step 4: Use the inverted mask to show the background (text appears where there are no objects)\n",
    "background = cv2.bitwise_and(img_cv2, img_cv2, mask=inverted_mask)  # Background (image) without objects\n",
    "\n",
    "# Step 5: Combine the background with the text where the mask is transparent\n",
    "result_with_text = cv2.add(background, text_layer)\n",
    "\n",
    "# Step 6: Now apply the original object mask on top of this image\n",
    "masked_image = cv2.bitwise_and(img_cv2, img_cv2, mask=object_mask)  # Extract only the objects\n",
    "\n",
    "# Step 7: Combine the masked image (objects) with the final image (text behind objects)\n",
    "final_result = cv2.add(result_with_text, masked_image)\n",
    "\n",
    "# Convert result image back to PIL for display\n",
    "result_image_pil = Image.fromarray(cv2.cvtColor(final_result, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "# Display the final image with text behind objects and polygons\n",
    "result_image_pil.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "steps wise working code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 e:\\Neko\\neko_WP_20180829_22_40_52_Pro.jpg: 640x384 2 persons, 1 couch, 1 potted plant, 1 vase, 997.4ms\n",
      "Speed: 0.0ms preprocess, 997.4ms inference, 22.5ms postprocess per image at shape (1, 3, 640, 384)\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "# Step 1: Load Image and YOLOv8 Model for Segmentation\n",
    "model = YOLO(\"yolov8m-seg.pt\")\n",
    "img_path = \"neko_WP_20180829_22_40_52_Pro.jpg\" #\"NekoImages\\\\pic.jpg\"\n",
    "\n",
    "# Predict the results from the image\n",
    "results = model.predict(img_path)\n",
    "\n",
    "# Extract results from the first prediction\n",
    "result = results[0]\n",
    "masks = result.masks\n",
    "\n",
    "# Load the image into OpenCV format\n",
    "img = Image.open(img_path)\n",
    "img_cv2 = np.array(img)\n",
    "img_cv2 = cv2.cvtColor(img_cv2, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "# Step 3: Create Masked Layer for Segmented Areas\n",
    "object_mask = np.zeros((img_cv2.shape[0], img_cv2.shape[1]), dtype=np.uint8)\n",
    "\n",
    "for mask_idx in range(len(masks)):\n",
    "    mask = masks[mask_idx].data[0].numpy()\n",
    "    mask = (mask * 255).astype(np.uint8)  # Convert to binary mask (0 or 255)\n",
    "\n",
    "    # Resize the mask to match the size of the original image\n",
    "    mask_resized = cv2.resize(mask, (img_cv2.shape[1], img_cv2.shape[0]), interpolation=cv2.INTER_NEAREST)\n",
    "    \n",
    "    # Add this mask to the overall object mask\n",
    "    object_mask = cv2.bitwise_or(object_mask, mask_resized)\n",
    "\n",
    "# Step 4: Add Text to Original Image and Mask Where Segments Appear\n",
    "# Text Overlay on the original image\n",
    "text_layer = np.zeros_like(img_cv2, dtype=np.uint8)\n",
    "text_position = (100, 500)  # Text position\n",
    "text = \"Family\"\n",
    "cv2.putText(text_layer, text, text_position, cv2.FONT_HERSHEY_SIMPLEX, 2, (255, 255, 255), 10)\n",
    "\n",
    "# Create an inverted mask of the segmented regions to hide text\n",
    "inverted_mask = cv2.bitwise_not(object_mask)\n",
    "\n",
    "# Combine text layer with the original image\n",
    "text_visible = cv2.addWeighted(img_cv2, 1, text_layer, 1, 0)\n",
    "\n",
    "# Step 5: Combine Segmented Layer with Text Hidden Under Segments\n",
    "# Apply the mask to ensure text is hidden behind segmented objects\n",
    "masked_image = cv2.bitwise_and(img_cv2, img_cv2, mask=object_mask)\n",
    "background = cv2.bitwise_and(text_visible, text_visible, mask=inverted_mask)\n",
    "final_image = cv2.add(masked_image, background)\n",
    "\n",
    "# Convert final image to PIL format for displaying\n",
    "final_image_pil = Image.fromarray(cv2.cvtColor(final_image, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "# Display final result\n",
    "final_image_pil.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 e:\\Neko\\neko_WP_20180829_22_40_52_Pro.jpg: 640x384 2 persons, 1 couch, 1 potted plant, 1 vase, 931.5ms\n",
      "Speed: 0.0ms preprocess, 931.5ms inference, 11.4ms postprocess per image at shape (1, 3, 640, 384)\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "# Step 1: Load Image and YOLOv8 Model for Segmentation\n",
    "model = YOLO(\"yolov8m-seg.pt\")\n",
    "img_path = \"neko_WP_20180829_22_40_52_Pro.jpg\"  # or your image path\n",
    "\n",
    "# Predict the results from the image\n",
    "results = model.predict(img_path)\n",
    "\n",
    "# Extract results from the first prediction\n",
    "result = results[0]\n",
    "masks = result.masks\n",
    "class_labels = result.names  # Get class labels\n",
    "classes_to_keep = [\"person\", \"dog\", \"cat\", \"horse\", \"sheep\", \"cow\"]  # Define classes to keep\n",
    "\n",
    "# Load the image into OpenCV format\n",
    "img = Image.open(img_path)\n",
    "img_cv2 = np.array(img)\n",
    "img_cv2 = cv2.cvtColor(img_cv2, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "# Step 3: Create Masked Layer for Segmented Areas\n",
    "object_mask = np.zeros((img_cv2.shape[0], img_cv2.shape[1]), dtype=np.uint8)\n",
    "\n",
    "# Iterate through the results and apply mask only for the selected classes\n",
    "for mask_idx in range(len(masks)):\n",
    "    class_idx = int(result.boxes.cls[mask_idx].item())  # Get class index for current mask\n",
    "    class_name = class_labels[class_idx]  # Get the class name\n",
    "\n",
    "    # Only process masks for person and animal classes\n",
    "    if class_name in classes_to_keep:\n",
    "        mask = masks[mask_idx].data[0].numpy()\n",
    "        mask = (mask * 255).astype(np.uint8)  # Convert to binary mask (0 or 255)\n",
    "\n",
    "        # Resize the mask to match the size of the original image\n",
    "        mask_resized = cv2.resize(mask, (img_cv2.shape[1], img_cv2.shape[0]), interpolation=cv2.INTER_NEAREST)\n",
    "        \n",
    "        # Add this mask to the overall object mask\n",
    "        object_mask = cv2.bitwise_or(object_mask, mask_resized)\n",
    "\n",
    "# Step 4: Add Text to Original Image and Mask Where Segments Appear\n",
    "# Text Overlay on the original image\n",
    "text_layer = np.zeros_like(img_cv2, dtype=np.uint8)\n",
    "text_position = (100, 500)  # Text position\n",
    "text = \"Family\"\n",
    "cv2.putText(text_layer, text, text_position, cv2.FONT_HERSHEY_SIMPLEX, 2, (255, 255, 255), 10)\n",
    "\n",
    "# Create an inverted mask of the segmented regions to hide text\n",
    "inverted_mask = cv2.bitwise_not(object_mask)\n",
    "\n",
    "# Combine text layer with the original image\n",
    "text_visible = cv2.addWeighted(img_cv2, 1, text_layer, 1, 0)\n",
    "\n",
    "# Step 5: Combine Segmented Layer with Text Hidden Under Segments\n",
    "# Apply the mask to ensure text is hidden behind segmented objects\n",
    "masked_image = cv2.bitwise_and(img_cv2, img_cv2, mask=object_mask)\n",
    "background = cv2.bitwise_and(text_visible, text_visible, mask=inverted_mask)\n",
    "final_image = cv2.add(masked_image, background)\n",
    "\n",
    "# Convert final image to PIL format for displaying\n",
    "final_image_pil = Image.fromarray(cv2.cvtColor(final_image, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "# Display final result\n",
    "final_image_pil.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape: (1918, 1077, 3)\n",
      "Mask shape (before resize): (1918, 1077)\n",
      "Mask shape (after resize): (1918, 1077, 3)\n"
     ]
    }
   ],
   "source": [
    "def beautify_segment(image, mask):\n",
    "        if image is None or mask is None:\n",
    "            return image  # Return the original image if no mask or image is provided\n",
    "\n",
    "        # Ensure the mask is resized to match the image dimensions\n",
    "        if mask.shape[:2] != image.shape[:2]:\n",
    "            mask = cv2.resize(mask, (image.shape[1], image.shape[0]), interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "        # Convert mask to a single channel if needed\n",
    "        if len(mask.shape) > 2:\n",
    "            mask = cv2.cvtColor(mask, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        # Convert mask to 8-bit unsigned integer type\n",
    "        mask = mask.astype(np.uint8)\n",
    "\n",
    "        # Debug prints\n",
    "        print(\"Image shape:\", image.shape)\n",
    "        print(\"Mask shape (before resize):\", mask.shape)\n",
    "\n",
    "        # Extract the masked area\n",
    "        masked_area = cv2.bitwise_and(image, image, mask=mask)\n",
    "        \n",
    "        # Apply bilateral filtering for edge-preserving smoothing\n",
    "        smoothed = cv2.bilateralFilter(masked_area, 5, 85, 85)\n",
    "        \n",
    "        # Slightly enhance contrast and brightness of the masked area (subtle beautification)\n",
    "        enhanced = cv2.convertScaleAbs(smoothed, alpha=1.1, beta=10)\n",
    "        \n",
    "        # Create a blurred mask for smooth edges\n",
    "        blurred_mask = cv2.GaussianBlur(mask, (5, 5), 1)\n",
    "        blurred_mask = blurred_mask.astype(np.float32) / 255.0\n",
    "\n",
    "        # Ensure blurred_mask has the same number of channels as enhanced\n",
    "        if len(blurred_mask.shape) == 2:\n",
    "            blurred_mask = cv2.cvtColor(blurred_mask, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "        # Debug prints\n",
    "        print(\"Mask shape (after resize):\", blurred_mask.shape)\n",
    "        \n",
    "        # Blend the enhanced region with the original image using the blurred mask\n",
    "        masked_correction = cv2.multiply(enhanced.astype(np.float32), blurred_mask)\n",
    "        original_contribution = cv2.multiply(image.astype(np.float32), 1 - blurred_mask)\n",
    "        blended_masked_area = cv2.add(masked_correction, original_contribution).astype(np.uint8)\n",
    "        \n",
    "        # Merge the processed masked region back to the original image\n",
    "        mask_inverse = cv2.bitwise_not(mask)\n",
    "        untouched_area = cv2.bitwise_and(image, image, mask=mask_inverse)\n",
    "        beautified_image = cv2.add(untouched_area, blended_masked_area)\n",
    "        \n",
    "        # Optionally enhance the entire image slightly\n",
    "        beautified_image = cv2.convertScaleAbs(beautified_image, alpha=1.05, beta=5)\n",
    "        \n",
    "        return beautified_image\n",
    "\n",
    "img_cv2 = beautify_segment(img_cv2, object_mask)\n",
    "\n",
    "# Convert final image to PIL format for displaying\n",
    "final_image_pil = Image.fromarray(cv2.cvtColor(img_cv2, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "# Display final result\n",
    "final_image_pil.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'sudo' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'sudo' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!sudo apt update\n",
    "!sudo apt install -y python3-pip openjdk-11-jdk git unzip zlib1g-dev lib32stdc++6 lib32z1\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
